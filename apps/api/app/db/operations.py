"""Database operations for document ingestion and storage"""

import logging
from typing import List, Tuple, Optional, Dict, Any
from uuid import UUID, uuid4
import asyncpg
from datetime import datetime

from app.core.config import get_settings
from app.services.chunking import ChunkData
from .session import get_db_pool

logger = logging.getLogger(__name__)


async def insert_chunks(document_id: str, chunks: List[ChunkData]) -> List[str]:
    """
    Bulk insert chunks into the database.
    
    Args:
        document_id: Document UUID
        chunks: List of ChunkData objects
        
    Returns:
        List of chunk IDs that were inserted
        
    Raises:
        asyncpg.PostgreSQLError: If database operation fails
    """
    if not chunks:
        return []
    
    pool = await get_db_pool()
    chunk_ids = []
    
    logger.info(
        "Starting chunk insertion",
        extra={
            "document_id": document_id,
            "chunk_count": len(chunks)
        }
    )
    
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Prepare batch data
            batch_data = []
            for chunk in chunks:
                chunk_id = str(uuid4())
                chunk_ids.append(chunk_id)
                
                # Create TSV (Text Search Vector) for full-text search
                # The tsv column is automatically generated by trigger in the schema
                batch_data.append((
                    chunk_id,
                    document_id,
                    chunk.content,
                    chunk.section_title,
                    chunk.page_number,
                    chunk.section_type,
                    chunk.token_count,
                    chunk.char_count,
                    chunk.metadata or {}
                ))
            
            # Bulk insert using executemany for better performance
            await conn.executemany(
                """
                INSERT INTO chunks (
                    id, document_id, content, section_title, page_number, 
                    section_type, token_count, char_count, metadata
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                """,
                batch_data
            )
    
    logger.info(
        "Chunk insertion completed",
        extra={
            "document_id": document_id,
            "chunks_inserted": len(chunk_ids)
        }
    )
    
    return chunk_ids


async def insert_embeddings(chunk_ids: List[str], embeddings: List[List[float]]) -> None:
    """
    Bulk insert embeddings into the database.
    
    Args:
        chunk_ids: List of chunk UUIDs
        embeddings: List of embedding vectors (1536-dimensional)
        
    Raises:
        asyncpg.PostgreSQLError: If database operation fails
        ValueError: If chunk_ids and embeddings length mismatch
    """
    if len(chunk_ids) != len(embeddings):
        raise ValueError(f"Length mismatch: {len(chunk_ids)} chunk IDs vs {len(embeddings)} embeddings")
    
    if not chunk_ids:
        return
    
    pool = await get_db_pool()
    
    logger.info(
        "Starting embedding insertion",
        extra={
            "chunk_count": len(chunk_ids),
            "embedding_dimension": len(embeddings[0]) if embeddings else 0
        }
    )
    
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Prepare batch data
            batch_data = [
                (str(uuid4()), chunk_id, embedding)
                for chunk_id, embedding in zip(chunk_ids, embeddings)
            ]
            
            # Bulk insert embeddings
            await conn.executemany(
                """
                INSERT INTO embeddings (id, chunk_id, embedding) 
                VALUES ($1, $2, $3)
                """,
                batch_data
            )
    
    logger.info(
        "Embedding insertion completed",
        extra={"embeddings_inserted": len(chunk_ids)}
    )


async def update_document_status(
    document_id: str, 
    status: str, 
    page_count: Optional[int] = None,
    chunks_count: Optional[int] = None,
    error_message: Optional[str] = None
) -> None:
    """
    Update document processing status.
    
    Args:
        document_id: Document UUID
        status: Status string ("processing", "completed", "failed")
        page_count: Number of pages in document (optional)
        chunks_count: Number of chunks created (optional)
        error_message: Error message if status is "failed" (optional)
    """
    pool = await get_db_pool()
    
    logger.info(
        "Updating document status",
        extra={
            "document_id": document_id,
            "status": status,
            "page_count": page_count,
            "chunks_count": chunks_count
        }
    )
    
    async with pool.acquire() as conn:
        # Update document status
        await conn.execute(
            """
            UPDATE documents 
            SET 
                status = $2,
                page_count = COALESCE($3, page_count),
                chunks_count = COALESCE($4, chunks_count),
                error_message = $5,
                updated_at = NOW()
            WHERE id = $1
            """,
            document_id, status, page_count, chunks_count, error_message
        )
    
    logger.info(
        "Document status updated",
        extra={"document_id": document_id, "new_status": status}
    )


async def get_document_chunks(document_id: str, user_id: str) -> List[Dict[str, Any]]:
    """
    Get all chunks for a document (with RLS check).
    
    Args:
        document_id: Document UUID
        user_id: User UUID (for RLS)
        
    Returns:
        List of chunk dictionaries
    """
    pool = await get_db_pool()
    
    async with pool.acquire() as conn:
        # Set user context for RLS
        await conn.execute("SELECT set_config('request.jwt.claims', $1, true)", f'{{"sub":"{user_id}"}}')
        
        rows = await conn.fetch(
            """
            SELECT c.id, c.content, c.section_title, c.page_number,
                   c.section_type, c.token_count, c.char_count, c.metadata,
                   c.created_at
            FROM chunks c
            JOIN documents d ON d.id = c.document_id
            WHERE c.document_id = $1
            ORDER BY c.created_at
            """,
            document_id
        )
        
        return [dict(row) for row in rows]


async def get_document_embeddings(document_id: str, user_id: str) -> List[Tuple[str, List[float]]]:
    """
    Get all embeddings for a document (with RLS check).
    
    Args:
        document_id: Document UUID
        user_id: User UUID (for RLS)
        
    Returns:
        List of (chunk_id, embedding) tuples
    """
    pool = await get_db_pool()
    
    async with pool.acquire() as conn:
        # Set user context for RLS
        await conn.execute("SELECT set_config('request.jwt.claims', $1, true)", f'{{"sub":"{user_id}"}}')
        
        rows = await conn.fetch(
            """
            SELECT e.chunk_id, e.embedding
            FROM embeddings e
            JOIN chunks c ON c.id = e.chunk_id
            JOIN documents d ON d.id = c.document_id
            WHERE c.document_id = $1
            ORDER BY c.created_at
            """,
            document_id
        )
        
        return [(row['chunk_id'], row['embedding']) for row in rows]


async def delete_document_chunks(document_id: str, user_id: str) -> int:
    """
    Delete all chunks and embeddings for a document (with RLS check).
    
    Args:
        document_id: Document UUID
        user_id: User UUID (for RLS)
        
    Returns:
        Number of chunks deleted
    """
    pool = await get_db_pool()
    
    async with pool.acquire() as conn:
        async with conn.transaction():
            # Set user context for RLS
            await conn.execute("SELECT set_config('request.jwt.claims', $1, true)", f'{{"sub":"{user_id}"}}')
            
            # Delete embeddings first (foreign key constraint)
            await conn.execute(
                """
                DELETE FROM embeddings 
                WHERE chunk_id IN (
                    SELECT c.id FROM chunks c 
                    JOIN documents d ON d.id = c.document_id 
                    WHERE c.document_id = $1
                )
                """,
                document_id
            )
            
            # Delete chunks
            result = await conn.execute(
                """
                DELETE FROM chunks 
                WHERE document_id = $1 AND document_id IN (
                    SELECT id FROM documents WHERE id = $1
                )
                """,
                document_id
            )
            
            # Extract count from result
            deleted_count = int(result.split()[-1]) if result else 0
            
    logger.info(
        "Document chunks deleted",
        extra={
            "document_id": document_id,
            "chunks_deleted": deleted_count
        }
    )
    
    return deleted_count


async def get_chunk_with_embedding(chunk_id: str, user_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a specific chunk with its embedding (with RLS check).
    
    Args:
        chunk_id: Chunk UUID
        user_id: User UUID (for RLS)
        
    Returns:
        Dictionary with chunk data and embedding, or None if not found
    """
    pool = await get_db_pool()
    
    async with pool.acquire() as conn:
        # Set user context for RLS
        await conn.execute("SELECT set_config('request.jwt.claims', $1, true)", f'{{"sub":"{user_id}"}}')
        
        row = await conn.fetchrow(
            """
            SELECT c.id, c.document_id, c.content, c.section_title, c.page_number,
                   c.section_type, c.token_count, c.char_count, c.metadata,
                   c.created_at, e.embedding
            FROM chunks c
            LEFT JOIN embeddings e ON e.chunk_id = c.id
            JOIN documents d ON d.id = c.document_id
            WHERE c.id = $1
            """,
            chunk_id
        )
        
        return dict(row) if row else None


async def count_document_chunks(document_id: str, user_id: str) -> int:
    """
    Count chunks for a document (with RLS check).
    
    Args:
        document_id: Document UUID
        user_id: User UUID (for RLS)
        
    Returns:
        Number of chunks
    """
    pool = await get_db_pool()
    
    async with pool.acquire() as conn:
        # Set user context for RLS
        await conn.execute("SELECT set_config('request.jwt.claims', $1, true)", f'{{"sub":"{user_id}"}}')
        
        row = await conn.fetchrow(
            """
            SELECT COUNT(*) as count
            FROM chunks c
            JOIN documents d ON d.id = c.document_id
            WHERE c.document_id = $1
            """,
            document_id
        )
        
        return row['count'] if row else 0